{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. pytorch两大法宝\n",
    "- dir(): 打开，看见\n",
    "- help(): 说明书"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T20:45:50.849639Z",
     "end_time": "2023-06-17T20:45:51.940718Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['__annotations__',\n '__builtins__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.cuda.is_available)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function is_available in module torch.cuda:\n",
      "\n",
      "is_available() -> bool\n",
      "    Returns a bool indicating if CUDA is currently available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.cuda.is_available)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 加载数据\n",
    "## 2.1 Dataset\n",
    "提供一种方式去获取数据及其label\n",
    "- 如何获取每一个数据及其label\n",
    "- 总共有多少个数据\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module torch.utils.data.dataset:\n",
      "\n",
      "class Dataset(typing.Generic)\n",
      " |  An abstract class representing a :class:`Dataset`.\n",
      " |  \n",
      " |  All datasets that represent a map from keys to data samples should subclass\n",
      " |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      " |  data sample for a given key. Subclasses could also optionally overwrite\n",
      " |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      " |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      " |  of :class:`~torch.utils.data.DataLoader`.\n",
      " |  \n",
      " |  .. note::\n",
      " |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      " |    sampler that yields integral indices.  To make it work with a map-style\n",
      " |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  __getitem__(self, index) -> +T_co\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "help(Dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, root_dir, label_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.path = os.path.join(root_dir, label_dir)  # 到目录级\n",
    "        self.img_path = os.listdir(self.path)  # 获取目录中所有文件名称\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):  # 返回img和label\n",
    "        img_name = self.img_path[idx]\n",
    "        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)\n",
    "        img = Image.open(img_item_path)\n",
    "        label = self.label_dir\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):  # 返回给定数据集的大小\n",
    "        return len(self.img_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "root_dir = r\"hymenoptera_data/train\"\n",
    "ants_label_dir = \"ants\"\n",
    "bees_label_dir = \"bees\"\n",
    "ants_dataset = MyData(root_dir, ants_label_dir)  # Dataset实例化\n",
    "bees_dataset = MyData(root_dir, bees_label_dir)  # Dataset实例化\n",
    "\n",
    "train_dataset = ants_dataset + bees_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "img, label = ants_dataset[0]\n",
    "img.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 DataLoader\n",
    "获取数据的时候进行打包，为网络提供不同的数据形式\n",
    "\n",
    "常用参数包括：\n",
    "- batch_size: 每次取几个数据\n",
    "- shuffle: 取数据是否打乱\n",
    "- num_workers: 多进程计算，默认为0\n",
    "- drop_last: 多出来的数据要不要舍去"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 3\n",
      "torch.Size([64, 3, 32, 32]) tensor([7, 1, 3, 6, 7, 9, 0, 6, 5, 1, 5, 1, 8, 6, 7, 3, 8, 6, 3, 3, 4, 1, 4, 8,\n",
      "        6, 2, 0, 3, 8, 2, 4, 0, 0, 3, 0, 0, 0, 5, 1, 1, 1, 6, 7, 1, 1, 4, 4, 7,\n",
      "        3, 0, 0, 1, 3, 5, 1, 7, 0, 1, 3, 7, 4, 5, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10('./dataset', train=False, transform=torchvision.transforms.ToTensor())\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "img, target = test_data[0]\n",
    "i = 0\n",
    "for data in test_loader:  # test_loader无法用[i]索引\n",
    "    imgs, targets = data\n",
    "    i = i + 1\n",
    "    if i > 0.5:\n",
    "        break\n",
    "        pass\n",
    "print(img.shape, target)\n",
    "print(imgs.shape, targets)\n",
    "\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "for epoch in range(2):\n",
    "    step = 0\n",
    "    for data in test_loader:\n",
    "        imgs, targets = data\n",
    "        writer.add_images('Epoch: {}'.format(epoch), imgs, step)\n",
    "        step = step + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Tensor Board使用\n",
    "## 3.1 写函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# help(SummaryWriter)\n",
    "writer = SummaryWriter(\"logs\")\n",
    "# writer.add_image()\n",
    "for i in range(100):\n",
    "    writer.add_scalar('y=x', i, i)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 写图像"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# help(SummaryWriter)\n",
    "writer = SummaryWriter(\"logs\")\n",
    "img_path = r\"hymenoptera_data/train/ants/0013035.jpg\"\n",
    "img_PIL = Image.open(img_path)\n",
    "img_array = np.array(img_PIL)\n",
    "print(img_array.shape, type(img_array))\n",
    "\n",
    "writer.add_image(tag='figure', img_tensor=img_array, dataformats=\"HWC\")\n",
    "# for i in range(100):\n",
    "#     writer.add_scalar('y=x', i, i)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Transform 使用\n",
    "\n",
    "一个工具箱，里面有很多工具，例如totensor, resize等。多用于处理图片。\n",
    "\n",
    "通过transforms.ToTensor解决两个问题：\n",
    "- transform怎么使用\n",
    "- 为什么我们需要Tensor数据类型\n",
    "\n",
    "学习方法:\n",
    "- 关注输入和输出类型\n",
    "- 多看官方文档\n",
    "- 关注方法需要什么参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 ToTensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "img_path = r\"hymenoptera_data/train/ants/5650366_e22b7e1065.jpg\"\n",
    "img = Image.open(img_path)\n",
    "cv_img = cv2.imread(img_path)\n",
    "# img.show()\n",
    "# print(img)\n",
    "\n",
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "tensor_img_cv = tensor_trans(cv_img)\n",
    "# tensor_img\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_image(tag='ToTensor', img_tensor=tensor_img, dataformats=\"CHW\",global_step=0)  # 两个图片颜色不一样，因为PIL和cv2用的颜色格式不同\n",
    "writer.add_image(tag='ToTensor', img_tensor=tensor_img_cv, dataformats=\"CHW\",global_step=1)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Normalize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_path = r\"hymenoptera_data/train/ants/5650366_e22b7e1065.jpg\"\n",
    "img = Image.open(img_path)\n",
    "\n",
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "trans_norm = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "norm_tensor_img = trans_norm(tensor_img)\n",
    "# tensor_img\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_image(tag='normal', img_tensor=tensor_img, dataformats=\"CHW\",global_step=0)  # 两个图片颜色不一样，因为PIL和cv2用的颜色格式不同\n",
    "writer.add_image(tag='normal', img_tensor=norm_tensor_img, dataformats=\"CHW\",global_step=1)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Resize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_path = r\"hymenoptera_data/train/ants/5650366_e22b7e1065.jpg\"\n",
    "img = Image.open(img_path)\n",
    "\n",
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "# trans_norm = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# norm_tensor_img = trans_norm(tensor_img)\n",
    "trans_resize = transforms.Resize([512, 512])\n",
    "resize_tensor_img = trans_resize(tensor_img)\n",
    "# tensor_img\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_image(tag='resize', img_tensor=tensor_img, dataformats=\"CHW\",global_step=0)  # 两个图片颜色不一样，因为PIL和cv2用的颜色格式不同\n",
    "writer.add_image(tag='resize', img_tensor=resize_tensor_img, dataformats=\"CHW\",global_step=1)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 Compose\n",
    "\n",
    "Compose就是把多个Transforms组合在一起使用。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "img_path = r\"hymenoptera_data/train/ants/5650366_e22b7e1065.jpg\"\n",
    "img = Image.open(img_path)\n",
    "# img.show()\n",
    "# print(img)\n",
    "\n",
    "tensor_trans = transforms.ToTensor()\n",
    "normal_trans = transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "resize_trans = transforms.Resize([512, 512])\n",
    "compose_trans = transforms.Compose([tensor_trans, normal_trans, resize_trans])\n",
    "\n",
    "trans_img = compose_trans(img)\n",
    "# tensor_img\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_image(tag='Compose', img_tensor=tensor_img, dataformats=\"CHW\",global_step=0)  # 两个图片颜色不一样，因为PIL和cv2用的颜色格式不同\n",
    "writer.add_image(tag='Compose', img_tensor=trans_img, dataformats=\"CHW\",global_step=1)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. 数据集的使用\n",
    "\n",
    "这里用torchvision为例进行说明"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(tensor([[[0.6196, 0.6235, 0.6471,  ..., 0.5373, 0.4941, 0.4549],\n",
      "         [0.5961, 0.5922, 0.6235,  ..., 0.5333, 0.4902, 0.4667],\n",
      "         [0.5922, 0.5922, 0.6196,  ..., 0.5451, 0.5098, 0.4706],\n",
      "         ...,\n",
      "         [0.2667, 0.1647, 0.1216,  ..., 0.1490, 0.0510, 0.1569],\n",
      "         [0.2392, 0.1922, 0.1373,  ..., 0.1020, 0.1137, 0.0784],\n",
      "         [0.2118, 0.2196, 0.1765,  ..., 0.0941, 0.1333, 0.0824]],\n",
      "\n",
      "        [[0.4392, 0.4353, 0.4549,  ..., 0.3725, 0.3569, 0.3333],\n",
      "         [0.4392, 0.4314, 0.4471,  ..., 0.3725, 0.3569, 0.3451],\n",
      "         [0.4314, 0.4275, 0.4353,  ..., 0.3843, 0.3725, 0.3490],\n",
      "         ...,\n",
      "         [0.4863, 0.3922, 0.3451,  ..., 0.3804, 0.2510, 0.3333],\n",
      "         [0.4549, 0.4000, 0.3333,  ..., 0.3216, 0.3216, 0.2510],\n",
      "         [0.4196, 0.4118, 0.3490,  ..., 0.3020, 0.3294, 0.2627]],\n",
      "\n",
      "        [[0.1922, 0.1843, 0.2000,  ..., 0.1412, 0.1412, 0.1294],\n",
      "         [0.2000, 0.1569, 0.1765,  ..., 0.1216, 0.1255, 0.1333],\n",
      "         [0.1843, 0.1294, 0.1412,  ..., 0.1333, 0.1333, 0.1294],\n",
      "         ...,\n",
      "         [0.6941, 0.5804, 0.5373,  ..., 0.5725, 0.4235, 0.4980],\n",
      "         [0.6588, 0.5804, 0.5176,  ..., 0.5098, 0.4941, 0.4196],\n",
      "         [0.6275, 0.5843, 0.5176,  ..., 0.4863, 0.5059, 0.4314]]]), 3)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "dataset_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./dataset', train=True, download=True, transform=dataset_transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./dataset', train=False, download=True, transform=dataset_transform)\n",
    "\n",
    "print(test_set[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. 网络搭建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 小试牛刀"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input + 1\n",
    "        return output\n",
    "\n",
    "tudui = Tudui()\n",
    "x = torch.tensor(1.0)\n",
    "print(tudui(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 卷积层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tudui(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10('./dataset', train=False, transform=torchvision.transforms.ToTensor(), download=False)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "tudui = Tudui()\n",
    "print(tudui)\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = tudui(imgs)\n",
    "    print(output.shape)\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 池化层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\"dataset\", train=False, download=False, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# input = torch.tensor([[1, 2, 0, 3, 1],\n",
    "#                       [0, 1, 2, 3, 1],\n",
    "#                       [1, 2, 1, 0, 0],\n",
    "#                       [5, 2, 3, 1, 1],\n",
    "#                       [2, 1, 0, 1, 1]], dtype=torch.float32)\n",
    "#\n",
    "# input = torch.reshape(input, (-1, 1, 5, 5))\n",
    "# print(input.shape)\n",
    "\n",
    "class Wu(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.maxpool1(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "wu = Wu()\n",
    "# output = wu(input)\n",
    "# print(output)\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    writer.add_images(\"input\", imgs, step)\n",
    "    output = wu(imgs)\n",
    "    writer.add_images(\"output\", output, step)\n",
    "    step = step + 1\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 非线性激活\n",
    "\n",
    "ReLU, Sigmoid ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5 线性层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\"dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=False)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "class Wu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        self.linear1 = Linear(196608, 10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.linear1(input)\n",
    "        return output\n",
    "\n",
    "wu = Wu()\n",
    "\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    print(imgs.shape)\n",
    "    # output = torch.reshape(imgs, (1, 1, 1, -1))  # 展平\n",
    "    output = torch.flatten(imgs)  # 展平，和上面的reshape本质上是一样的，但更快更推荐\n",
    "    print(output.shape)\n",
    "    output = wu(output)\n",
    "    print(output.shape)\n",
    "\n",
    "    # 这里报错是因为最后一个batch只有16个（其他都有64个）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.6 nn搭建实践 —— 以CIFAR10为例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (model1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Wu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        self.conv1 = Conv2d(3, 32, 5, 1, 2)\n",
    "        self.maxpool1 = MaxPool2d(2, ceil_mode=True)  # celi_model用于设置余数要不要算进来计算，True是要的意思\n",
    "        self.conv2 = Conv2d(32, 32, 5, 1, 2)\n",
    "        self.maxpool2 = MaxPool2d(2, ceil_mode=True)\n",
    "        self.conv3 = Conv2d(32, 64, 5, 1, 2)\n",
    "        self.maxpool3 = MaxPool2d(2, ceil_mode=True)\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(1024, 64)\n",
    "        self.linear2 = Linear(64, 10)\n",
    "\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Conv2d(32, 32, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Conv2d(32, 64, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "\n",
    "        x = self.model1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "wu = Wu()\n",
    "print(wu)\n",
    "\n",
    "# 测试网络的正确性\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "# output = wu(input)\n",
    "# print(output.shape)\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_graph(wu, input)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. 损失函数、反向传播、优化\n",
    "\n",
    "## 7.1 Loss函数 和 backward()\n",
    "\n",
    "- 计算实际输出和目标之间的差距\n",
    "- 为我们更新输出提供一定的依据（反向传播）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2848, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3066, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3045, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2874, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3014, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3103, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3099, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3069, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3064, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3154, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3071, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2923, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2969, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2920, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3023, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3011, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3033, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3043, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3003, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3070, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3066, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3011, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2963, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3063, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3001, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3082, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3038, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3065, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3117, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3135, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3057, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3133, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3096, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3099, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2965, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3014, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3085, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3078, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3071, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3058, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3149, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3166, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3106, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3059, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3046, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3048, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3092, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3087, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3039, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2996, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3055, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3088, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3125, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3023, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3046, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3011, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3040, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3092, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3061, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2987, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3083, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2951, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2993, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2957, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3061, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3141, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3043, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3118, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2945, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3103, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3031, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3090, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3026, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3002, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2945, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3055, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3106, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3140, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3124, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2970, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2965, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3192, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3156, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3076, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2957, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3127, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3176, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3094, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2968, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3048, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2938, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3098, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3109, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3126, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3158, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2961, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2996, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3045, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2990, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2996, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3053, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3039, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3042, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3009, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3079, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2966, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3080, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3103, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2977, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3038, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3102, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3101, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3125, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3089, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3091, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3107, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3041, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3053, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3056, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3003, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3121, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3131, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3009, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3004, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2889, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2948, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2986, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3045, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3062, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3051, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3112, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3094, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2936, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3111, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3010, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3051, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3041, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3040, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3119, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3007, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3048, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3157, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3185, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3234, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3210, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2980, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3313, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3080, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3052, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3185, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3068, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2911, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3136, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3020, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3257, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3145, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3050, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2959, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3151, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2991, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3090, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3069, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3096, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3324, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2899, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3213, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3140, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3208, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3044, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3144, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3003, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2969, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3070, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3116, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3253, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2972, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3001, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2996, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3004, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3123, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3051, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3072, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3089, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3001, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3116, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3013, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3228, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2989, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2961, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3143, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3127, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3162, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3224, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2907, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3081, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2897, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3016, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3091, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3163, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3027, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3150, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3234, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3164, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3360, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3246, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3184, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3066, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3172, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3160, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3221, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2944, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3168, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3208, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3167, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3027, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3221, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3033, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3080, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3319, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2883, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3251, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2980, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3141, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3220, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3030, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2949, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3312, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3081, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2966, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3036, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2998, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3013, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3266, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3178, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3142, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3194, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2972, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3093, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2920, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3130, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2847, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3198, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2944, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3102, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3121, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3183, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3123, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3199, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2988, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2933, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2943, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3072, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3199, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3244, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3048, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3069, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2946, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2962, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3122, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3140, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3059, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2945, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3092, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2984, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3261, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3111, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2957, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3130, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3068, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3118, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3136, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3109, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3191, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3136, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2983, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3029, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3005, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2978, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3027, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2996, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3117, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3246, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3188, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3164, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3193, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3167, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2923, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2934, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3258, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3037, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.2944, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor(2.3177, grad_fn=<NllLossBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\"dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=False)\n",
    "\n",
    "dataloader = DataLoader(dataset, 64, True, drop_last=False)\n",
    "\n",
    "class Wu(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        # self.conv1 = Conv2d(3, 32, 5, 1, 2)\n",
    "        # self.maxpool1 = MaxPool2d(2, ceil_mode=True)  # celi_model用于设置余数要不要算进来计算，True是要的意思\n",
    "        # self.conv2 = Conv2d(32, 32, 5, 1, 2)\n",
    "        # self.maxpool2 = MaxPool2d(2, ceil_mode=True)\n",
    "        # self.conv3 = Conv2d(32, 64, 5, 1, 2)\n",
    "        # self.maxpool3 = MaxPool2d(2, ceil_mode=True)\n",
    "        # self.flatten = Flatten()\n",
    "        # self.linear1 = Linear(1024, 64)\n",
    "        # self.linear2 = Linear(64, 10)\n",
    "\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Conv2d(32, 32, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Conv2d(32, 64, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "\n",
    "        x = self.model1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "wu = Wu()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    # print(imgs.shape, targets.shape)\n",
    "    output = wu(imgs)\n",
    "    # print(output)\n",
    "    result_loss = loss(output, targets)\n",
    "    # print(output.shape)\n",
    "    print(result_loss)\n",
    "    result_loss.backward()  # 反向传播求梯度\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 优化器\n",
    "\n",
    "这一节包括很多超参数的设置。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(361.0699, grad_fn=<AddBackward0>)\n",
      "tensor(357.8331, grad_fn=<AddBackward0>)\n",
      "tensor(342.2478, grad_fn=<AddBackward0>)\n",
      "tensor(320.7163, grad_fn=<AddBackward0>)\n",
      "tensor(313.1638, grad_fn=<AddBackward0>)\n",
      "tensor(304.2767, grad_fn=<AddBackward0>)\n",
      "tensor(296.5301, grad_fn=<AddBackward0>)\n",
      "tensor(288.4012, grad_fn=<AddBackward0>)\n",
      "tensor(280.5760, grad_fn=<AddBackward0>)\n",
      "tensor(273.1507, grad_fn=<AddBackward0>)\n",
      "tensor(266.6805, grad_fn=<AddBackward0>)\n",
      "tensor(262.1871, grad_fn=<AddBackward0>)\n",
      "tensor(257.3079, grad_fn=<AddBackward0>)\n",
      "tensor(252.0457, grad_fn=<AddBackward0>)\n",
      "tensor(248.0773, grad_fn=<AddBackward0>)\n",
      "tensor(244.2853, grad_fn=<AddBackward0>)\n",
      "tensor(240.3637, grad_fn=<AddBackward0>)\n",
      "tensor(237.6151, grad_fn=<AddBackward0>)\n",
      "tensor(234.4866, grad_fn=<AddBackward0>)\n",
      "tensor(229.6207, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\"dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=False)\n",
    "\n",
    "dataloader = DataLoader(dataset, 64, True, drop_last=False)\n",
    "\n",
    "class Wu(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        # self.conv1 = Conv2d(3, 32, 5, 1, 2)\n",
    "        # self.maxpool1 = MaxPool2d(2, ceil_mode=True)  # ceil_model用于设置余数要不要算进来计算，True是要的意思\n",
    "        # self.conv2 = Conv2d(32, 32, 5, 1, 2)\n",
    "        # self.maxpool2 = MaxPool2d(2, ceil_mode=True)\n",
    "        # self.conv3 = Conv2d(32, 64, 5, 1, 2)\n",
    "        # self.maxpool3 = MaxPool2d(2, ceil_mode=True)\n",
    "        # self.flatten = Flatten()\n",
    "        # self.linear1 = Linear(1024, 64)\n",
    "        # self.linear2 = Linear(64, 10)\n",
    "\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Conv2d(32, 32, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Conv2d(32, 64, 5, 1, 2),\n",
    "            MaxPool2d(2, ceil_mode=True),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "\n",
    "        x = self.model1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "wu = Wu()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(wu.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0  # 看每一轮的loss是多少\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        output = wu(imgs)\n",
    "        result_loss = loss(output, targets)\n",
    "        optim.zero_grad()\n",
    "        result_loss.backward()\n",
    "        optim.step()\n",
    "        # print(result_loss)\n",
    "        running_loss = running_loss + result_loss\n",
    "    print(running_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. 现有模型的使用与修改——迁移学习"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (add_linear): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "vgg16_false = torchvision.models.vgg16(pretrained=False)\n",
    "vgg16_true = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "print(vgg16_false)\n",
    "\n",
    "# 添加Modeule：\n",
    "vgg16_true.add_module('add_linear', nn.Linear(1000,10))\n",
    "\n",
    "# 修改Module：\n",
    "vgg16_false.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)\n",
    "\n",
    "print(vgg16_true)\n",
    "print(vgg16_false)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. 网络模型的保存与加载"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.1 模型的保存"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# 方法一：\n",
    "torch.save(vgg16, \"vgg16_method1.pth\")\n",
    "\n",
    "# 方法二：\n",
    "torch.save(vgg16.state_dict(), \"vgg16_method2.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.2 模型的加载"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\62411\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 方法一：\n",
    "model1 = torch.load(\"vgg16_method1.pth\")\n",
    "print(model1)\n",
    "\n",
    "# 放法二：\n",
    "model2 = torchvision.models.vgg16(pretrained=False)  # 要先重新搭一遍模型的框架\n",
    "model2.load_state_dict(torch.load(\"vgg16_method2.pth\"))\n",
    "print(model2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. 模型整体"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "训练数据集的长度为：50000\n",
      "测试数据集的长度为：10000\n",
      "第1轮训练开始\n",
      "训练次数:100, Loss:2.28983736038208\n",
      "训练次数:200, Loss:2.292184829711914\n",
      "训练次数:300, Loss:2.2539641857147217\n",
      "训练次数:400, Loss:2.2014808654785156\n",
      "训练次数:500, Loss:2.149015188217163\n",
      "训练次数:600, Loss:2.056422710418701\n",
      "训练次数:700, Loss:1.921677827835083\n",
      "整体数据集上的Loss：2.1426024436950684\n",
      "整体测试集上的正确率: 0.29100000858306885\n",
      "本轮训练时长: 55.716891050338745\n",
      "第2轮训练开始\n",
      "训练次数:800, Loss:1.8396490812301636\n",
      "训练次数:900, Loss:1.9348477125167847\n",
      "训练次数:1000, Loss:1.8176124095916748\n",
      "训练次数:1100, Loss:1.8490245342254639\n",
      "训练次数:1200, Loss:1.751731514930725\n",
      "训练次数:1300, Loss:1.6793042421340942\n",
      "训练次数:1400, Loss:1.8811403512954712\n",
      "训练次数:1500, Loss:1.841321587562561\n",
      "整体数据集上的Loss：1.628587245941162\n",
      "整体测试集上的正确率: 0.38679999113082886\n",
      "本轮训练时长: 56.8618278503418\n",
      "第3轮训练开始\n",
      "训练次数:1600, Loss:1.7468445301055908\n",
      "训练次数:1700, Loss:1.7796196937561035\n",
      "训练次数:1800, Loss:1.9693225622177124\n",
      "训练次数:1900, Loss:1.696186900138855\n",
      "训练次数:2000, Loss:1.7908596992492676\n",
      "训练次数:2100, Loss:1.514176607131958\n",
      "训练次数:2200, Loss:1.5048952102661133\n",
      "训练次数:2300, Loss:1.4097715616226196\n",
      "整体数据集上的Loss：1.7135692834854126\n",
      "整体测试集上的正确率: 0.39149999618530273\n",
      "本轮训练时长: 58.404699087142944\n",
      "第4轮训练开始\n",
      "训练次数:2400, Loss:1.7035685777664185\n",
      "训练次数:2500, Loss:1.6275163888931274\n",
      "训练次数:2600, Loss:1.6576471328735352\n",
      "训练次数:2700, Loss:1.7182128429412842\n",
      "训练次数:2800, Loss:1.4597941637039185\n",
      "训练次数:2900, Loss:1.4353801012039185\n",
      "训练次数:3000, Loss:1.4979275465011597\n",
      "训练次数:3100, Loss:1.4263513088226318\n",
      "整体数据集上的Loss：1.41921865940094\n",
      "整体测试集上的正确率: 0.46709999442100525\n",
      "本轮训练时长: 61.53332448005676\n",
      "第5轮训练开始\n",
      "训练次数:3200, Loss:1.5747426748275757\n",
      "训练次数:3300, Loss:1.4467087984085083\n",
      "训练次数:3400, Loss:1.4386578798294067\n",
      "训练次数:3500, Loss:1.6064103841781616\n",
      "训练次数:3600, Loss:1.470504879951477\n",
      "训练次数:3700, Loss:1.6007413864135742\n",
      "训练次数:3800, Loss:1.4807047843933105\n",
      "训练次数:3900, Loss:1.211030125617981\n",
      "整体数据集上的Loss：1.3711649179458618\n",
      "整体测试集上的正确率: 0.4408000111579895\n",
      "本轮训练时长: 58.43860912322998\n",
      "第6轮训练开始\n",
      "训练次数:4000, Loss:1.5880506038665771\n",
      "训练次数:4100, Loss:1.3572982549667358\n",
      "训练次数:4200, Loss:1.178848385810852\n",
      "训练次数:4300, Loss:1.2856087684631348\n",
      "训练次数:4400, Loss:1.8011119365692139\n",
      "训练次数:4500, Loss:1.2314386367797852\n",
      "训练次数:4600, Loss:1.2171469926834106\n",
      "整体数据集上的Loss：2.187568426132202\n",
      "整体测试集上的正确率: 0.4916999936103821\n",
      "本轮训练时长: 66.08713746070862\n",
      "第7轮训练开始\n",
      "训练次数:4700, Loss:1.3767420053482056\n",
      "训练次数:4800, Loss:1.4164940118789673\n",
      "训练次数:4900, Loss:1.5056548118591309\n",
      "训练次数:5000, Loss:1.4362995624542236\n",
      "训练次数:5100, Loss:1.2009259462356567\n",
      "训练次数:5200, Loss:1.0672577619552612\n",
      "训练次数:5300, Loss:1.4002344608306885\n",
      "训练次数:5400, Loss:1.4862149953842163\n",
      "整体数据集上的Loss：0.9101048707962036\n",
      "整体测试集上的正确率: 0.5181000232696533\n",
      "本轮训练时长: 62.94953656196594\n",
      "第8轮训练开始\n",
      "训练次数:5500, Loss:1.236939549446106\n",
      "训练次数:5600, Loss:1.3224211931228638\n",
      "训练次数:5700, Loss:1.4407240152359009\n",
      "训练次数:5800, Loss:1.5174065828323364\n",
      "训练次数:5900, Loss:1.4086204767227173\n",
      "训练次数:6000, Loss:1.000179409980774\n",
      "训练次数:6100, Loss:1.4464633464813232\n",
      "训练次数:6200, Loss:1.0950804948806763\n",
      "整体数据集上的Loss：1.3791985511779785\n",
      "整体测试集上的正确率: 0.551800012588501\n",
      "本轮训练时长: 66.01333665847778\n",
      "第9轮训练开始\n",
      "训练次数:6300, Loss:1.2006875276565552\n",
      "训练次数:6400, Loss:1.280002474784851\n",
      "训练次数:6500, Loss:1.2263861894607544\n",
      "训练次数:6600, Loss:1.2835328578948975\n",
      "训练次数:6700, Loss:1.3687678575515747\n",
      "训练次数:6800, Loss:0.9958192706108093\n",
      "训练次数:6900, Loss:1.1094297170639038\n",
      "训练次数:7000, Loss:1.4156924486160278\n",
      "整体数据集上的Loss：1.3877546787261963\n",
      "整体测试集上的正确率: 0.5394999980926514\n",
      "本轮训练时长: 72.34738492965698\n",
      "第10轮训练开始\n",
      "训练次数:7100, Loss:0.9599012136459351\n",
      "训练次数:7200, Loss:1.1854043006896973\n",
      "训练次数:7300, Loss:1.311025857925415\n",
      "训练次数:7400, Loss:1.3145456314086914\n",
      "训练次数:7500, Loss:1.0440195798873901\n",
      "训练次数:7600, Loss:1.2353557348251343\n",
      "训练次数:7700, Loss:1.1696563959121704\n",
      "训练次数:7800, Loss:1.0839238166809082\n",
      "整体数据集上的Loss：1.0715763568878174\n",
      "整体测试集上的正确率: 0.5871000289916992\n",
      "本轮训练时长: 57.56394982337952\n",
      "第11轮训练开始\n",
      "训练次数:7900, Loss:1.2586249113082886\n",
      "训练次数:8000, Loss:1.191507339477539\n",
      "训练次数:8100, Loss:1.119042992591858\n",
      "训练次数:8200, Loss:1.181315541267395\n",
      "训练次数:8300, Loss:1.3143647909164429\n",
      "训练次数:8400, Loss:1.1122208833694458\n",
      "训练次数:8500, Loss:1.1020845174789429\n",
      "训练次数:8600, Loss:0.9702326655387878\n",
      "整体数据集上的Loss：0.8999680280685425\n",
      "整体测试集上的正确率: 0.5613999962806702\n",
      "本轮训练时长: 58.0785710811615\n",
      "第12轮训练开始\n",
      "训练次数:8700, Loss:0.8345033526420593\n",
      "训练次数:8800, Loss:1.0345790386199951\n",
      "训练次数:8900, Loss:1.0393322706222534\n",
      "训练次数:9000, Loss:0.9107018709182739\n",
      "训练次数:9100, Loss:1.2106856107711792\n",
      "训练次数:9200, Loss:1.100280523300171\n",
      "训练次数:9300, Loss:0.9490389823913574\n",
      "整体数据集上的Loss：1.5492522716522217\n",
      "整体测试集上的正确率: 0.5975000262260437\n",
      "本轮训练时长: 58.06660461425781\n",
      "第13轮训练开始\n",
      "训练次数:9400, Loss:0.9506456255912781\n",
      "训练次数:9500, Loss:1.042384147644043\n",
      "训练次数:9600, Loss:0.872729480266571\n",
      "训练次数:9700, Loss:1.2110356092453003\n",
      "训练次数:9800, Loss:0.975554347038269\n",
      "训练次数:9900, Loss:1.0893853902816772\n",
      "训练次数:10000, Loss:1.1443902254104614\n",
      "训练次数:10100, Loss:0.7573027610778809\n",
      "整体数据集上的Loss：1.0276312828063965\n",
      "整体测试集上的正确率: 0.6107000112533569\n",
      "本轮训练时长: 63.81322455406189\n",
      "第14轮训练开始\n",
      "训练次数:10200, Loss:0.8886916041374207\n",
      "训练次数:10300, Loss:1.017903447151184\n",
      "训练次数:10400, Loss:0.9261404275894165\n",
      "训练次数:10500, Loss:0.9106543064117432\n",
      "训练次数:10600, Loss:1.2394322156906128\n",
      "训练次数:10700, Loss:0.9271863102912903\n",
      "训练次数:10800, Loss:1.0764223337173462\n",
      "训练次数:10900, Loss:1.002593755722046\n",
      "整体数据集上的Loss：0.6460890769958496\n",
      "整体测试集上的正确率: 0.6169999837875366\n",
      "本轮训练时长: 58.466532468795776\n",
      "第15轮训练开始\n",
      "训练次数:11000, Loss:1.101599097251892\n",
      "训练次数:11100, Loss:0.9584475159645081\n",
      "训练次数:11200, Loss:0.8547614216804504\n",
      "训练次数:11300, Loss:0.6253002285957336\n",
      "训练次数:11400, Loss:1.0839585065841675\n",
      "训练次数:11500, Loss:0.869784951210022\n",
      "训练次数:11600, Loss:1.1278791427612305\n",
      "训练次数:11700, Loss:1.2889460325241089\n",
      "整体数据集上的Loss：0.9053001403808594\n",
      "整体测试集上的正确率: 0.6187000274658203\n",
      "本轮训练时长: 57.81727075576782\n",
      "第16轮训练开始\n",
      "训练次数:11800, Loss:0.7709137797355652\n",
      "训练次数:11900, Loss:1.0008864402770996\n",
      "训练次数:12000, Loss:0.9636265635490417\n",
      "训练次数:12100, Loss:0.871795117855072\n",
      "训练次数:12200, Loss:1.2381153106689453\n",
      "训练次数:12300, Loss:0.8434767723083496\n",
      "训练次数:12400, Loss:0.7105555534362793\n",
      "训练次数:12500, Loss:0.8798430562019348\n",
      "整体数据集上的Loss：1.4790329933166504\n",
      "整体测试集上的正确率: 0.620199978351593\n",
      "本轮训练时长: 57.92398405075073\n",
      "第17轮训练开始\n",
      "训练次数:12600, Loss:0.8573326468467712\n",
      "训练次数:12700, Loss:0.9921228289604187\n",
      "训练次数:12800, Loss:0.9143497943878174\n",
      "训练次数:12900, Loss:0.9040340781211853\n",
      "训练次数:13000, Loss:1.3648325204849243\n",
      "训练次数:13100, Loss:0.9273669123649597\n",
      "训练次数:13200, Loss:0.8282896876335144\n",
      "整体数据集上的Loss：1.810578465461731\n",
      "整体测试集上的正确率: 0.5940999984741211\n",
      "本轮训练时长: 58.491466760635376\n",
      "第18轮训练开始\n",
      "训练次数:13300, Loss:0.9408126473426819\n",
      "训练次数:13400, Loss:0.8800665140151978\n",
      "训练次数:13500, Loss:0.7113040685653687\n",
      "训练次数:13600, Loss:0.7308441400527954\n",
      "训练次数:13700, Loss:0.8794403672218323\n",
      "训练次数:13800, Loss:1.0848467350006104\n",
      "训练次数:13900, Loss:0.7727599143981934\n",
      "训练次数:14000, Loss:0.8994053602218628\n",
      "整体数据集上的Loss：1.2808321714401245\n",
      "整体测试集上的正确率: 0.6237000226974487\n",
      "本轮训练时长: 57.94393062591553\n",
      "第19轮训练开始\n",
      "训练次数:14100, Loss:0.7571915984153748\n",
      "训练次数:14200, Loss:0.9049302935600281\n",
      "训练次数:14300, Loss:0.6676451563835144\n",
      "训练次数:14400, Loss:0.7459707260131836\n",
      "训练次数:14500, Loss:0.6059356927871704\n",
      "训练次数:14600, Loss:0.6908125877380371\n",
      "训练次数:14700, Loss:0.9490706324577332\n",
      "训练次数:14800, Loss:0.8460461497306824\n",
      "整体数据集上的Loss：0.5948205590248108\n",
      "整体测试集上的正确率: 0.6424000263214111\n",
      "本轮训练时长: 58.30895400047302\n",
      "第20轮训练开始\n",
      "训练次数:14900, Loss:0.9319303035736084\n",
      "训练次数:15000, Loss:0.8825535178184509\n",
      "训练次数:15100, Loss:0.6353074312210083\n",
      "训练次数:15200, Loss:0.8216386437416077\n",
      "训练次数:15300, Loss:0.8117018938064575\n",
      "训练次数:15400, Loss:0.8288928866386414\n",
      "训练次数:15500, Loss:0.8887328505516052\n",
      "训练次数:15600, Loss:0.9735478162765503\n",
      "整体数据集上的Loss：1.2299046516418457\n",
      "整体测试集上的正确率: 0.6413999795913696\n",
      "本轮训练时长: 58.357823848724365\n"
     ]
    }
   ],
   "source": [
    "# 第一种方法：gpu训练三处加cuda() 分别是：网络模型、损失函数、数据\n",
    "# 第二种方法：device = 'cuda' 也是三处：网络模型、损失函数、数据\n",
    "\n",
    "import torch.optim.optimizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# from model import *\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\"dataset\", train=True, transform=torchvision.transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"dataset\", train=False, transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "\n",
    "train_data_size = len(train_data)  # 直接获取数据集的大小\n",
    "test_data_size = len(test_data)\n",
    "print(\"训练数据集的长度为：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度为：{}\".format(test_data_size))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, 64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, 64, shuffle=True)\n",
    "\n",
    "\n",
    "class Wu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 64),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "wu = Wu()\n",
    "if torch.cuda.is_available():\n",
    "    # wu = wu.cuda()  # gpu训练1\n",
    "    wu = wu.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    # loss_fn = loss_fn.cuda()  # gpu训练2\n",
    "    loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(wu.parameters(), lr=0.01)\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "epoch = 20\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"第{}轮训练开始\".format(i + 1))\n",
    "    start_time = time.time()\n",
    "    wu.train()\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            # imgs = imgs.cuda()  # gpu训练3\n",
    "            # targets = targets.cuda()  # gpu训练3\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "        outputs = wu(imgs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # 优化器优化模型\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (total_train_step + 1) % 100 == 0:\n",
    "            print(\"训练次数:{}, Loss:{}\".format(total_train_step + 1, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "        total_train_step = total_train_step + 1\n",
    "\n",
    "    wu.eval()\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                # imgs = imgs.cuda()  # gpu训练3\n",
    "                # targets = targets.cuda()  # gpu训练3\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "            outputs = wu(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_test_loss = total_test_loss + loss\n",
    "            total_accuracy = total_accuracy + (outputs.argmax(1) == targets).sum()\n",
    "    end_time = time.time()\n",
    "    print(\"整体数据集上的Loss：{}\".format(loss.item()))\n",
    "    print(\"整体测试集上的正确率: {}\".format(total_accuracy / test_data_size))\n",
    "    print(\"本轮训练时长: {}\".format(end_time-start_time))\n",
    "    writer.add_scalar(\"test_loss\", loss.item(), total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n",
    "    total_test_step = total_test_step + 1\n",
    "\n",
    "    # torch.save(wu, \"wu_{}.pth\".format(i))  # 保存每一次训练模型\n",
    "    # torch.save(wu.state_dict(), \"wu_{}\".format(i))  # 另一种保存方式\n",
    "    # print(\"模型已保存！\")\n",
    "\n",
    "torch.save(wu.state_dict(), \"wu.pth\")\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T19:53:31.209541Z",
     "end_time": "2023-06-19T20:13:36.869978Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. 模型验证"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=276x182 at 0x192C542A560>\n",
      "torch.Size([3, 32, 32])\n",
      "Wu(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-7.3367, -2.9395, -0.0099,  4.1078,  5.0973,  6.2311, -0.7234,  2.6055,\n",
      "         -7.2523, -0.3435]])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "image_path = 'dog2.jpg'\n",
    "image = Image.open(image_path)\n",
    "print(image)\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image = transforms(image)\n",
    "print(image.shape)\n",
    "\n",
    "class Wu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wu, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 64),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    pass\n",
    "\n",
    "model = Wu()\n",
    "model.load_state_dict(torch.load(\"wu.pth\"))\n",
    "print(model)\n",
    "\n",
    "image = torch.reshape(image, (1, 3, 32, 32))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "print(output)\n",
    "print(output.argmax(1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T20:24:01.867454Z",
     "end_time": "2023-06-19T20:24:01.903356Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
